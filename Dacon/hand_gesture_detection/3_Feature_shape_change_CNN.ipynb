{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Feature_shape_change_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JL4vY78VnHyk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Google Colab 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed 고정\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def seed_everything(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "seed_everything(2022)"
      ],
      "metadata": {
        "id": "9FF9B6UOnMuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/data/train.csv') # 2335 rows 34 columns\n",
        "test = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/data/test.csv')   # 9343 rows 33 columns\n",
        "submission = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "8f1OAWmnnasz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기존 32개 칼럼 + 짝수홀수 순서의 동일 32개 칼럼\n",
        "def feat_transform_8x8(train, test):\n",
        "    col_list = list(train.columns[1:-1])\n",
        "    X = train.iloc[:, 1:-1]     # sensor_1 ~ sensor_32 / [2335, 32]\n",
        "    target = test.iloc[:, 1:]   # sensor_1 ~ sensor_32 / [9343, 32]\n",
        "\n",
        "    for i in range(0, len(col_list), 2):\n",
        "        new = f'copy_{col_list[i]}'\n",
        "        X[new] = X[col_list[i]]\n",
        "\n",
        "    for i in range(1, len(col_list), 2):\n",
        "        new = f'copy_{col_list[i]}'\n",
        "        X[new] = X[col_list[i]]\n",
        "\n",
        "    for i in range(0, len(col_list), 2):\n",
        "        new = f'copy_{col_list[i]}'\n",
        "        target[new] = target[col_list[i]]\n",
        "\n",
        "    for i in range(1, len(col_list), 2):\n",
        "        new = f'copy_{col_list[i]}'\n",
        "        target[new] = target[col_list[i]]\n",
        "    \n",
        "    return X, target"
      ],
      "metadata": {
        "id": "MLzqAmHKsheQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fake sensor 4개 추가하고  6 * 6 형태로 진행\n",
        "\n",
        "def feat_transform_6x6(train, test):\n",
        "    col_list = list(train.columns[1:-1])\n",
        "    first_col = col_list[0:4]\n",
        "    second_col = col_list[4:10]\n",
        "    third_col = col_list[10:16]\n",
        "    fourth_col = col_list[16:22]\n",
        "    fifth_col = col_list[22:28]\n",
        "    sixth_col = col_list[28:32]\n",
        "\n",
        "    # fake sensor\n",
        "    fake_sensor_2 = [float(-150) for i in range(2335)]\n",
        "    fake_sensor_3 = [float(-150) for i in range(2335)]\n",
        "    fake_sensor_4 = [float(-150) for i in range(2335)]\n",
        "\n",
        "    my_train = pd.DataFrame({'fake_sensor_1':[float(-150) for i in range(2335)], })\n",
        "    for i in first_col:\n",
        "        my_train[i] = train[i]\n",
        "    my_train['fake_sensor_2'] = fake_sensor_2\n",
        "    for i in col_list[4:28]:\n",
        "        my_train[i] = train[i]\n",
        "    my_train['fake_sensor_3'] = fake_sensor_3\n",
        "    for i in sixth_col:\n",
        "        my_train[i] = train[i]\n",
        "    my_train['fake_sensor_4'] = fake_sensor_4\n",
        "    my_train['target'] = train['target']\n",
        "\n",
        "    my_test = pd.DataFrame({'fake_sensor_1':[float(-150) for i in range(2335)], })\n",
        "    for i in first_col:\n",
        "        my_test[i] = test[i]\n",
        "    my_test['fake_sensor_2'] = fake_sensor_2\n",
        "    for i in col_list[4:28]:\n",
        "        my_test[i] = test[i]\n",
        "    my_test['fake_sensor_3'] = fake_sensor_3\n",
        "    for i in sixth_col:\n",
        "        my_test[i] = test[i]\n",
        "    my_test['fake_sensor_4'] = fake_sensor_4\n",
        "\n",
        "    X = my_train.iloc[:, :-1]\n",
        "    target = my_test.iloc[:, :]\n",
        "    \n",
        "    return X, target"
      ],
      "metadata": {
        "id": "17gbDvz1vtAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X).reshape(-1, 8, 8, 1)                # [2335, 8, 8, 1]\n",
        "target = np.array(target).reshape(-1, 8, 8, 1)      # [9343, 8, 8, 1]"
      ],
      "metadata": {
        "id": "WhglrZfvvUml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder(sparse = False)\n",
        "y = ohe.fit_transform(train[['target']])    # [2335, 4]\n",
        "skf = StratifiedShuffleSplit(n_splits=10, train_size=0.9, test_size=0.1, random_state=2022)\n",
        "es = EarlyStopping(monitor = 'val_acc', patience = 10, mode = 'max', verbose = 1)"
      ],
      "metadata": {
        "id": "HN756DYhnamg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "record_list = list()\n",
        "\n",
        "for i in range(10):\n",
        "    cnn_acc = []\n",
        "    cnn_pred = np.zeros((target.shape[0], 4))   # [9343, 4]\n",
        "\n",
        "    dim_3_3 = 64\n",
        "    dim_1_1 = 16\n",
        "\n",
        "    for i, (tr_idx, val_idx) in enumerate(skf.split(X, train.target)) :\n",
        "        print(f'{i + 1} Fold Training.....')\n",
        "        tr_x, tr_y = X[tr_idx], y[tr_idx]\n",
        "        val_x, val_y = X[val_idx], y[val_idx]\n",
        "        \n",
        "        ### CNN 모델\n",
        "        cnn = Sequential([\n",
        "                        Conv2D(dim_3_3, (2, 2), padding = \"same\", activation = 'relu', input_shape = (8, 8, 1)),\n",
        "                        BatchNormalization(),\n",
        "                        Conv2D(dim_1_1, (1, 1), padding = \"same\", activation = 'relu'),\n",
        "                        BatchNormalization(),\n",
        "                        Conv2D(dim_3_3, (3, 3), padding = \"same\", activation = 'relu'),\n",
        "                        \n",
        "                        BatchNormalization(),\n",
        "                        Conv2D(dim_1_1, (1, 1), padding = \"same\", activation = 'relu'),\n",
        "                        BatchNormalization(),\n",
        "                        Conv2D(dim_3_3, (4, 4), padding = \"same\", activation = 'relu'),\n",
        "                        BatchNormalization(),\n",
        "                        GlobalAveragePooling2D(),\n",
        "                        Dense(32, activation = 'relu'),\n",
        "                        Dense(4, activation = 'softmax')\n",
        "                        ])\n",
        "\n",
        "\n",
        "        ### ModelCheckPoint Fold마다 갱신\n",
        "        mc = ModelCheckpoint(f'model_{i + 1}.h5', save_best_only = True, monitor = 'val_acc', mode = 'auto', verbose = 0)    # monitor 변경 / val_acc\n",
        "        \n",
        "        ### 모델 compile\n",
        "        cnn.compile(optimizer = RMSprop(learning_rate = 0.0004), loss = 'categorical_crossentropy', metrics = ['acc'])            # optimizer 변경\n",
        "\n",
        "        cnn.fit(tr_x, tr_y, validation_data = (val_x, val_y), epochs = 100, batch_size = 32, callbacks = [es, mc], verbose = 0)\n",
        "\n",
        "        ### 최고 성능 기록 모델 Load\n",
        "        best = load_model(f'model_{i + 1}.h5')\n",
        "        ### validation predict\n",
        "        val_pred = best.predict(val_x)\n",
        "        ### 확률값 중 최대값을 클래스로 매칭\n",
        "        val_cls = np.argmax(val_pred, axis = 1)\n",
        "        ### Fold별 정확도 산출\n",
        "        fold_cnn_acc = accuracy_score(np.argmax(val_y, axis = 1), val_cls)\n",
        "        cnn_acc.append(fold_cnn_acc)\n",
        "        print(f'{i + 1} Fold ACC of CNN = {fold_cnn_acc}\\n')\n",
        "\n",
        "        ### Fold별 test 데이터에 대한 예측값 생성 및 앙상블\n",
        "        fold_pred = best.predict(target) / skf.n_splits\n",
        "        # print(fold_pred.shape)\n",
        "        cnn_pred += fold_pred\n",
        "    record_list.append(cnn_pred)"
      ],
      "metadata": {
        "id": "U10Y1XkQnakI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 확인\n",
        "print(np.mean(cnn_acc)) \n",
        "print(np.argmax(cnn_pred, axis = 1))\n",
        "cnn_pred"
      ],
      "metadata": {
        "id": "coOhUWoRnaiO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}