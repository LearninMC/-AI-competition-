{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wandb_MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb  # wandb 설치"
      ],
      "metadata": {
        "id": "LhssRZ30CHky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "ZwsT6sUzCMlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNDuOu_NB6tb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Google Colab 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import os\n",
        "import math\n",
        "import random"
      ],
      "metadata": {
        "id": "PNQVCTo0CNhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용할 GPU 지정\n",
        "print(\"number of GPUs: \", torch.cuda.device_count())\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"Does GPU exist? : \", use_cuda)\n",
        "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "metadata": {
        "id": "1XJsHh1mCT-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed 고정\n",
        "def seed_everything(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "seed_everything(2022)"
      ],
      "metadata": {
        "id": "sijlwlzvCBpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data 읽기\n",
        "train = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/data/train.csv') # 2335 rows 34 columns\n",
        "test = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/data/test.csv')   # 9343 rows 33 columns\n",
        "submission = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/data/sample_submission.csv')"
      ],
      "metadata": {
        "id": "9gJd6-1UCCqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP는 data 검증용으로 사용\n",
        "\n",
        "sub2 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_2.csv')\n",
        "sub3 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_3.csv')\n",
        "sub4 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_4.csv')\n",
        "sub5 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_5.csv')\n",
        "sub6 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_6.csv')\n",
        "sub7 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_7.csv')\n",
        "sub8 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_8.csv')\n",
        "sub9 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_9.csv')\n",
        "sub10 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_10.csv')\n",
        "sub11 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_11.csv')\n",
        "sub12 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_12.csv')\n",
        "sub13 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_13.csv')\n",
        "sub17 = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_17.csv')\n",
        "\n",
        "\n",
        "same_inference = sub2[(sub2['target'] == sub3['target']) & (sub2['target'] == sub4['target']) & (sub2['target'] == sub5['target'])\n",
        "     & (sub2['target'] == sub6['target']) & (sub2['target'] == sub7['target']) & (sub2['target'] == sub8['target'])\n",
        "      & (sub2['target'] == sub9['target']) & (sub2['target'] == sub10['target']) & (sub2['target'] == sub11['target'])\n",
        "       & (sub2['target'] == sub12['target']) & (sub2['target'] == sub13['target']) & (sub2['target'] == sub17['target'])]\n",
        "\n",
        "same_idx = list(same_inference['id']-1)\n",
        "new_train = test.loc[same_idx]\n",
        "new_train['target'] = list(same_inference['target'])\n",
        "\n",
        "# train + test 행 병합 후 scaling\n",
        "all_data = pd.concat([train.iloc[:,1:-1], test.iloc[:,1:]])\n",
        "\n",
        "# min max scaling\n",
        "mins = all_data.min()\n",
        "maxs = all_data.max()\n",
        "\n",
        "all_minmax = (all_data - mins) / (maxs - mins)\n",
        "all_minmax.describe()\n",
        "\n",
        "# Train 데이터 X,y  /  Test 데이터 X 나눠주기\n",
        "X = all_minmax.iloc[:9280, :]   # new_train 9280  11678/ original = 2335\n",
        "y = train.iloc[:, -1]\n",
        "\n",
        "test_X = all_minmax.iloc[9280:, :]  # new_train 9280  11678/ original = 2335"
      ],
      "metadata": {
        "id": "NPxkYMilCDpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class my_dataset(Dataset):\n",
        "    \n",
        "    def __init__(self, X_data, y_data):\n",
        "        self.X_data = X_data\n",
        "        self.y_data = y_data\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        return self.X_data[index], self.y_data[index]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.X_data)"
      ],
      "metadata": {
        "id": "zBbfl88pCDnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(My_Model, self).__init__()\n",
        "        \n",
        "        self.Layer_1 = nn.Linear(num_features, 64)  # origin = (num_features, 32) / 64\n",
        "        self.Layer_2 = nn.Linear(64, 64)            # origin = (32, 16)           / (64, 128)\n",
        "        self.Layer_3 = nn.Linear(64, 32)            # origin = (16, 8)            / (32, 16)\n",
        "\n",
        "        self.BatchNorm_1 = nn.BatchNorm1d(64)       # origin = 32 / 64\n",
        "        self.BatchNorm_2 = nn.BatchNorm1d(64)       # origin = 16 / 32\n",
        "\n",
        "        self.Layer_out = nn.Linear(32, num_classes) # origin = (8, num_classes) / 16\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        x = self.Layer_1(inputs)\n",
        "        x = self.BatchNorm_1(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.Layer_2(x)\n",
        "        x = self.BatchNorm_2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.Layer_3(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.Layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "Hl-pHQlsCDlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        first_cycle_steps (int): First cycle step size.\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 first_cycle_steps: int,\n",
        "                 cycle_mult: float = 1.,\n",
        "                 max_lr: float = 0.1,\n",
        "                 min_lr: float = 0.001,\n",
        "                 warmup_steps: int = 0,\n",
        "                 gamma: float = 1.,\n",
        "                 last_epoch: int = -1\n",
        "                 ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "\n",
        "        self.first_cycle_steps = first_cycle_steps  # first cycle step size\n",
        "        self.cycle_mult = cycle_mult  # cycle steps magnification\n",
        "        self.base_max_lr = max_lr  # first max learning rate\n",
        "        self.max_lr = max_lr  # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr  # min learning rate\n",
        "        self.warmup_steps = warmup_steps  # warmup step size\n",
        "        self.gamma = gamma  # decrease rate of max learning rate by cycle\n",
        "\n",
        "        self.cur_cycle_steps = first_cycle_steps  # first cycle step size\n",
        "        self.cycle = 0  # cycle count\n",
        "        self.step_in_cycle = last_epoch  # step size of the current cycle\n",
        "\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "        # set learning rate min_lr\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr for base_lr in\n",
        "                    self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int(\n",
        "                    (self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(\n",
        "                        self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "\n",
        "        self.max_lr = self.base_max_lr * (self.gamma ** self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "g_U6j_42CDti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weight(model, kind='xavier'):\n",
        "    for name, i in model.named_parameters():\n",
        "        if kind == 'xavier':\n",
        "            if i.dim() < 2:\n",
        "                continue\n",
        "            if 'weight' in name:\n",
        "                init.xavier_normal_(i, gain=1.0)\n",
        "            elif 'bias' in name:\n",
        "                init.xavier_uniform_(i, gain=1.0)\n",
        "            else:\n",
        "                pass\n",
        "        elif kind == 'kaiming':\n",
        "            if i.dim() < 2:\n",
        "                continue\n",
        "            if 'weight' in name:\n",
        "                init.kaiming_normal_(i)\n",
        "            elif 'bias' in name:\n",
        "                init.kaiming_uniform_(i)\n",
        "            else:\n",
        "                pass"
      ],
      "metadata": {
        "id": "5WPCZv_iCDrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder(sparse = False)\n",
        "y = ohe.fit_transform(train[['target']])    \n",
        "new_y = (y+0.05)/1.2                        # label smoothing\n",
        "y = new_y"
      ],
      "metadata": {
        "id": "sgs9vgoECDi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login --relogin"
      ],
      "metadata": {
        "id": "5EEOO998DNyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'seed':2022,\n",
        "    'fold':5, # 학습시킬 fold\n",
        "    'n_split':5, # fold 개수\n",
        "    'batch_size': 1024, # 1024\n",
        "    'num_classes': 4,\n",
        "    'epoch': 100,\n",
        "    'model_name': \"MLP\", \n",
        "    'initialization': \"kaiming\", # kaiming, xavier\n",
        "    'color': 'rgb'\n",
        "}\n",
        "\n",
        "wandb.config = CFG\n",
        "experiment_name = 'label_smooth_enhance_training'"
      ],
      "metadata": {
        "id": "3AZ4_u62CDgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_pred = np.zeros([9343, 4])\n",
        "mlp_acc = list()\n",
        "accuracy_stats_list = list()\n",
        "loss_stats_list = list()\n",
        "loss_flag = False\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "\n",
        "    # wandb 기록시작\n",
        "    run = wandb.init(project=f\"{CFG['model_name']}\", settings=wandb.Settings(start_method=\"thread\"), name=f\"{experiment_name}_{CFG['n_split']}split_{fold}\")\n",
        "\n",
        "    print(f'\\n --------------- Begin  Fold {fold+1} Training !! ---------------- \\n')\n",
        "    \n",
        "    loss_list = []\n",
        "    acc_list = []\n",
        "\n",
        "    best_acc = 0    # val_acc 가장 높은 모델로 test 예측 진행\n",
        "    best_loss = 999999\n",
        "    \n",
        "    X_train = X.iloc[tr_idx,:]\n",
        "    y_train = y[tr_idx]\n",
        "\n",
        "    X_val = X.iloc[val_idx,:]\n",
        "    y_val = y[val_idx]\n",
        "\n",
        "    train_dataset = my_dataset(torch.FloatTensor(X_train.to_numpy()), torch.FloatTensor(y_train))\n",
        "    valid_dataset = my_dataset(torch.FloatTensor(X_val.to_numpy()), torch.FloatTensor(y_val))\n",
        "\n",
        "    cosine_annealing_scheduler_arg = dict(\n",
        "        first_cycle_steps=len(train_dataset)//CFG['batch_size'] * CFG['epoch'] / 2, # CFG['epoch']\n",
        "        cycle_mult=2.0,\n",
        "        max_lr=4e-03,   #4e-05 / lr3 = 3e-04\n",
        "        min_lr=1e-06,\n",
        "        warmup_steps=len(train_dataset)//CFG['batch_size'] * 3,\n",
        "        gamma=0.7   # 0.9\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=CFG['batch_size'], shuffle=True, pin_memory=True)\n",
        "    valid_loader = DataLoader(dataset=valid_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
        "\n",
        "    model = My_Model(num_features=32, num_classes=4)\n",
        "    init_weight(model, kind='kaiming')\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0)\n",
        "    scheduler = CosineAnnealingWarmupRestarts(optimizer, **cosine_annealing_scheduler_arg)\n",
        "\n",
        "    if use_cuda:\n",
        "        model = model.to(DEVICE)\n",
        "        criterion = criterion.to(DEVICE)\n",
        "    \n",
        "    for epoch in range(CFG['epoch']):\n",
        "\n",
        "        train_losses = 0\n",
        "        train_mean_acc = 0\n",
        "\n",
        "        model.train()\n",
        "        for i, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
        "\n",
        "            X_train_batch, y_train_batch = X_train_batch.to(DEVICE), y_train_batch.to(DEVICE)\n",
        "            _y_train_batch = deepcopy(y_train_batch)\n",
        "\n",
        "            #Forward \n",
        "            y_train_output = model(X_train_batch)    # train loader로부터 나오는것도 gpu상으로올려줘야함\n",
        "            y_train_pred_softmax = torch.log_softmax(y_train_output, dim = 1) \n",
        "            _, y_train_pred_tags = torch.max(y_train_pred_softmax, dim = 1) # y_pred = torch.max(y_output, 1)[1] -> y_pred_tags\n",
        "\n",
        "            train_acc = accuracy_score(y_train_pred_tags.data.cpu(), np.argmax(_y_train_batch.data.cpu(), axis=1))\n",
        "            train_mean_acc += train_acc\n",
        "\n",
        "            train_loss = criterion(y_train_output.to(DEVICE), y_train_batch)\n",
        "            train_losses += train_loss\n",
        "            \n",
        "            loss_list.append(train_loss.item())\n",
        "            acc_list.append(train_acc)\n",
        "\n",
        "            #Backward\n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "             \n",
        "        valid_losses = 0\n",
        "        valid_mean_acc = 0\n",
        "\n",
        "        valid_y_pred = []\n",
        "        valid_acc_list = []\n",
        "        with torch.no_grad():\n",
        "\n",
        "            model.eval()\n",
        "            for X_valid_batch, y_valid_batch in valid_loader:  \n",
        "\n",
        "                X_valid_batch, y_valid_batch = X_valid_batch.to(DEVICE), y_valid_batch.to(DEVICE)\n",
        "                _y_valid_batch = deepcopy(y_valid_batch)\n",
        "\n",
        "                y_valid_output = model(X_valid_batch)\n",
        "                y_valid_pred_softmax = torch.log_softmax(y_valid_output, dim = 1)\n",
        "                _, y_valid_pred_tags = torch.max(y_valid_pred_softmax, dim = 1)\n",
        "\n",
        "                valid_acc = accuracy_score(y_valid_pred_tags.data.cpu(), np.argmax(_y_valid_batch.data.cpu(), axis=1))\n",
        "                valid_loss = criterion(y_valid_output, y_valid_batch)\n",
        "                valid_losses += valid_loss\n",
        "\n",
        "                \n",
        "                valid_acc_list.append(valid_acc)\n",
        "        \n",
        "        valid_acc = np.mean(valid_acc_list)\n",
        "        \n",
        "\n",
        "        if best_acc < valid_acc:  \n",
        "            print(f'best model changed! val_acc = {valid_acc} train_acc = {train_acc}') \n",
        "            best_model = deepcopy(model.state_dict())\n",
        "            best_acc = valid_acc\n",
        "\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            print(f'Fold [{fold+1}/{skf.n_splits}] Epoch [{epoch+1}/{CFG[\"epoch\"]}] Step [{i+1}/{len(train_loader)}] Loss: [{train_loss.item():.4f}] Train ACC [{train_acc*100:.2f}%] Valid ACC: [{valid_acc*100:.2f}%]')\n",
        "        \n",
        "        # wandb 기록\n",
        "        wandb_dict = {\n",
        "            'train loss': train_losses / len(train_loader),\n",
        "            'train acc': train_mean_acc / len(train_loader),\n",
        "            'valid loss': valid_losses / len(valid_loader),\n",
        "            'valid acc': valid_acc,\n",
        "            'learning rate': scheduler.get_lr()[0]\n",
        "            }\n",
        "\n",
        "        wandb.log(wandb_dict)\n",
        "\n",
        "    print(f'@@@ {fold + 1} best model prediction !! @@@')        \n",
        "    test_output = model(torch.FloatTensor(test_X.iloc[:,:].to_numpy()).to(DEVICE))\n",
        "    mlp_pred += (test_output.cpu().detach().numpy()/skf.n_splits)   # y_test_pred_softmax / test_output"
      ],
      "metadata": {
        "id": "aNpSn4-dCDec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과확인\n",
        "a = torch.FloatTensor(mlp_pred)\n",
        "b = torch.softmax(a, dim = 1)\n",
        "c = np.argmax(b, axis = 1)\n",
        "\n",
        "submission['target'] = c\n",
        "submission.target.value_counts()"
      ],
      "metadata": {
        "id": "wy-oYbEWCDU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전 데이터들과 비교\n",
        "sub_compare = pd.read_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_12.csv')\n",
        "submission[submission['target']!=sub_compare['target']]"
      ],
      "metadata": {
        "id": "wifrJsarCDSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과 제출\n",
        "submission.to_csv('/content/drive/MyDrive/AI_individual/Dacon_hand_gesture/submission/submit_19.csv', index = False)"
      ],
      "metadata": {
        "id": "xG7xSMnaD2mu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}