{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "김준철)train_hate_.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "votVine9eIMm"
      },
      "outputs": [],
      "source": [
        "# !pip install konlpy\n",
        "# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
        "# !pip install soynlp\n",
        "# !pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "from attrdict import AttrDict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from transformers import logging, get_linear_schedule_with_warmup\n",
        "\n",
        "from transformers import ( \n",
        "    BertConfig,\n",
        "    ElectraConfig\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizer,  \n",
        "    AutoTokenizer,\n",
        "    ElectraTokenizer,\n",
        "    AlbertTokenizer,\n",
        "    RobertaTokenizer\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    BertModel,\n",
        "    AutoModelForSequenceClassification, \n",
        "    ElectraForSequenceClassification,\n",
        "    BertForSequenceClassification,\n",
        "    AlbertForSequenceClassification,\n",
        "    RobertaForSequenceClassification\n",
        ")\n",
        "\n",
        "# preprocessing\n",
        "import re\n",
        "from soynlp.normalizer import *\n",
        "from pykospacing import Spacing\n",
        "from konlpy.tag import *"
      ],
      "metadata": {
        "id": "9yKOivzcfMhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)"
      ],
      "metadata": {
        "id": "iNpmIqCBrfIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hugging face models\n",
        "# preprocess\n",
        "# input\n",
        "# dropout\n",
        "# learning rate\n",
        "# epoch"
      ],
      "metadata": {
        "id": "xbBGJmxzBnmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "ps8rBC_wuiJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(x): # konlpy\n",
        "    return processor.morphs(x)\n",
        "\n",
        "def clean_punc(x): # normalize\n",
        "    punc_map = {\"“\": '\"', \"‘\": \"'\", '”':'\"', \"’\":\"'\", \"♥\":\" 사랑 \"}\n",
        "    tmp = ''\n",
        "    for st in x:\n",
        "        if st in punc_map.keys():\n",
        "            tmp += punc_map[st]\n",
        "        else:\n",
        "            tmp += st\n",
        "    return tmp\n",
        "\n",
        "def remove_bracket(x): # bracket 내 의미없는 것 제거\n",
        "    p1 = re.compile(r\"(\\[.{,9}\\])\")\n",
        "    tmp = p1.sub('', x)\n",
        "    p2 = re.compile(r\"\\([(종합)(인터뷰)(전문)]+\\)\")\n",
        "    return p2.sub('', tmp)\n",
        "\n",
        "def make_space(x):\n",
        "    # cnt = 0\n",
        "    # if len(x) > 20:\n",
        "    #     for i in x:\n",
        "    #         if i == ' ':\n",
        "    #             cnt += 1\n",
        "    # else:\n",
        "    #     cnt = 5\n",
        "    # if cnt < 5:\n",
        "    #     return spacing(x)\n",
        "    # else:\n",
        "    #     return x\n",
        "    return spacing(x)\n",
        "\n",
        "def remove_repeat(x):\n",
        "    p = re.compile(r\"[⁉️+❎+☝+(( ° ͜ʖ͡°)╭∩╮)+ᆢ+(^„^)+♨+(\\.){2}]\")\n",
        "    x = p.sub(' ', x)\n",
        "    return repeat_normalize(x, num_repeats=3)\n",
        "\n",
        "def remove_doubledot(x):\n",
        "    return x.replace('ᆢ', '')\n",
        "\n",
        "def remove_space(x):\n",
        "    p = re.compile(r\"[\\s+]\")\n",
        "    return p.sub(' ', x)"
      ],
      "metadata": {
        "id": "AduAjfwfujyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "cgPrDRxCngJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len):\n",
        "        self.title = dataset[:,0]\n",
        "        self.comment = dataset[:,1]\n",
        "        self.labels = dataset[:,2]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        title = self.title[idx]\n",
        "        comment = self.comment[idx]\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        tokenized_text = self.tokenizer(title, comment,\n",
        "                             padding= 'max_length',\n",
        "                             max_length=128,\n",
        "                             truncation=True,\n",
        "                             return_token_type_ids=True,\n",
        "                             return_attention_mask=True,\n",
        "                             return_tensors = \"pt\",\n",
        "                             is_split_into_words=True)\n",
        "        \n",
        "        data = {'input_ids': tokenized_text['input_ids'].clone().detach().long(),\n",
        "               'attention_mask': tokenized_text['attention_mask'].clone().detach().long(),\n",
        "               'token_type_ids': tokenized_text['token_type_ids'].clone().detach().long(),\n",
        "               }\n",
        "        \n",
        "        return data, label\n"
      ],
      "metadata": {
        "id": "dyQH0sfH7p8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_stratified_shuffle_split(df, fold, n_split, seed):\n",
        "    skf = StratifiedShuffleSplit(n_splits=n_split, train_size=0.8, test_size=0.2, random_state=seed)\n",
        "    for idx, i in enumerate(skf.split(df[['title', 'comment','label']], df['label'])):\n",
        "        if idx == fold:\n",
        "            train_data = df[['title', 'comment','label']].values[i[0]]\n",
        "            valid_data = df[['title', 'comment','label']].values[i[1]]\n",
        "    return train_data, valid_data"
      ],
      "metadata": {
        "id": "uj_789EHuKso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "LzKbfaZ3oSN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, hidden_size = 768, num_classes=None, selected_layers=False, params=None):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.model = BASE_MODELS[CFG['architecture']].from_pretrained(CFG['pretrained_model'], \n",
        "                                                         num_labels = CFG['num_classes'], \n",
        "                                                         output_attentions = False, # Whether the model returns attentions weights.\n",
        "                                                         output_hidden_states = True # Whether the model returns all hidden-states.\n",
        "                                                        )\n",
        "        self.softmax = nn.Softmax(dim=1) \n",
        "        self.selected_layers = selected_layers\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "\n",
        "    def forward(self, token_ids, attention_mask, segment_ids):      \n",
        "        outputs = self.model(input_ids = token_ids, \n",
        "                             token_type_ids = segment_ids.long(), \n",
        "                             attention_mask = attention_mask.float().to(token_ids.device))\n",
        "\n",
        "        if self.selected_layers == True:\n",
        "            hidden_states = outputs.hidden_states\n",
        "            pooled_output = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
        "\n",
        "            pooled_output = pooled_output[:, 0, :]\n",
        "            pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "            Model = nn.Linear(pooled_output.shape[1], self.num_classes).to(token_ids.device)\n",
        "            logits = Model(pooled_output)\n",
        "        \n",
        "        else:\n",
        "            logits=outputs.logits\n",
        "            \n",
        "        prob= self.softmax(logits)\n",
        "        return logits, prob "
      ],
      "metadata": {
        "id": "Pq5WMO9doff2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EarlyStopper"
      ],
      "metadata": {
        "id": "NP_gyV8uvQZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopper():\n",
        "    def __init__(self, patience: int)-> None:\n",
        "        self.patience = patience\n",
        "        self.patience_counter = 0\n",
        "        self.max_score = 0.\n",
        "        self.stop = False\n",
        "        self.save_model = False\n",
        "\n",
        "    def check_early_stopping(self, score: float)-> None:\n",
        "\n",
        "        if self.max_score == 0.:\n",
        "            self.max_score = score\n",
        "           \n",
        "        elif score <= self.max_score:\n",
        "            self.patience_counter += 1\n",
        "            if self.patience_counter == self.patience:\n",
        "                self.stop = True\n",
        "            self.save_model = False\n",
        "            print(f\"Early stopping counter {self.patience_counter}/{self.patience}\")\n",
        "        elif score > self.max_score:\n",
        "            self.patience_counter = 0\n",
        "            self.max_score = score\n",
        "            self.save_model = True\n",
        "\n",
        "            print(f\"Validation score increased {self.max_score} -> {score}\")"
      ],
      "metadata": {
        "id": "einYXrvMo8F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "mZL30vpivlSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class F1_Loss(nn.Module):\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "    def forward(self, y_pred, y_true,):\n",
        "        assert y_pred.ndim == 2\n",
        "        assert y_true.ndim == 1\n",
        "        y_true = F.one_hot(y_true, 2).to(torch.float32)\n",
        "        y_pred = F.softmax(y_pred, dim=1)\n",
        "        \n",
        "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
        "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
        "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
        "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
        "        return 1 - f1.mean()"
      ],
      "metadata": {
        "id": "D8QbqFrzysRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLoss(_Loss):\n",
        "    def __init__(self): \n",
        "        super(MyLoss, self).__init__()\n",
        "        self.lossCE = nn.CrossEntropyLoss() # weight=torch.tensor([1-(3646/(4721+3646)), 1-(4721/(4721+3646))]\n",
        "        self.lossF1 = F1_Loss()\n",
        "        \n",
        "    def forward(self, preds, trg):\n",
        "        # return (self.lossCE(preds, trg) + self.lossF1(preds, trg)) / 2\n",
        "        return self.lossCE(preds, trg)"
      ],
      "metadata": {
        "id": "VotPzdOgytWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scheduler"
      ],
      "metadata": {
        "id": "t-ve1XY41IUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        first_cycle_steps (int): First cycle step size.\n",
        "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
        "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
        "        min_lr(float): Min learning rate. Default: 0.001.\n",
        "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
        "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
        "        last_epoch (int): The index of last epoch. Default: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer: torch.optim.Optimizer,\n",
        "                 first_cycle_steps: int,\n",
        "                 cycle_mult: float = 1.,\n",
        "                 max_lr: float = 0.1,\n",
        "                 min_lr: float = 0.001,\n",
        "                 warmup_steps: int = 0,\n",
        "                 gamma: float = 1.,\n",
        "                 last_epoch: int = -1\n",
        "                 ):\n",
        "        assert warmup_steps < first_cycle_steps\n",
        "\n",
        "        self.first_cycle_steps = first_cycle_steps  # first cycle step size\n",
        "        self.cycle_mult = cycle_mult  # cycle steps magnification\n",
        "        self.base_max_lr = max_lr  # first max learning rate\n",
        "        self.max_lr = max_lr  # max learning rate in the current cycle\n",
        "        self.min_lr = min_lr  # min learning rate\n",
        "        self.warmup_steps = warmup_steps  # warmup step size\n",
        "        self.gamma = gamma  # decrease rate of max learning rate by cycle\n",
        "\n",
        "        self.cur_cycle_steps = first_cycle_steps  # first cycle step size\n",
        "        self.cycle = 0  # cycle count\n",
        "        self.step_in_cycle = last_epoch  # step size of the current cycle\n",
        "\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr for base_lr in\n",
        "                    self.base_lrs]\n",
        "        else:\n",
        "            return [base_lr + (self.max_lr - base_lr) \\\n",
        "                    * (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) \\\n",
        "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle = self.step_in_cycle + 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int(\n",
        "                    (self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(\n",
        "                        self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "\n",
        "        self.max_lr = self.base_max_lr * (self.gamma ** self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "3l3dOk3k1MfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Function"
      ],
      "metadata": {
        "id": "iUVYHfeZpqG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, criterion, train_loader, optimizer1, scheduler1, device): # optimizer2, scheduler2, \n",
        "    model.train()\n",
        "\n",
        "    train_losses = 0.\n",
        "    train_acc = 0.\n",
        "    train_score = 0.\n",
        "    \n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    for src, trg in tqdm(train_loader):\n",
        "        \n",
        "        mask = src['attention_mask'].to(device)\n",
        "        input_id = src['input_ids'].squeeze(1).to(device)\n",
        "        segment_ids = src['token_type_ids'].squeeze(1).to(device)\n",
        "        trg = trg.long().to(device)  \n",
        "        \n",
        "        output = model(input_id, mask, segment_ids)\n",
        "        loss = criterion(output[0].view(-1,2), trg.view(-1))\n",
        "        train_losses += loss.item()\n",
        "\n",
        "        acc = (output[0].argmax(dim=-1) == trg).sum().item()\n",
        "        train_acc += acc\n",
        "\n",
        "        preds.extend(output[0].argmax(dim=-1).detach().cpu().numpy())\n",
        "        targets.extend(trg.detach().cpu().numpy())\n",
        "        \n",
        "        optimizer1.zero_grad()\n",
        "        # optimizer2.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        # optimizer2.step()\n",
        "        scheduler1.step()\n",
        "        # scheduler2.step()\n",
        "    \n",
        "    train_score = f1_score(targets, preds)\n",
        "    \n",
        "    print(\"recall\", recall_score(targets, preds), \"precision\", precision_score(targets, preds))\n",
        "    print(accuracy_score(targets, preds))\n",
        "    return train_losses, train_acc, train_score"
      ],
      "metadata": {
        "id": "ULAgJE8Qy7BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_one_epoch(model, criterion, valid_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    valid_losses = 0.\n",
        "    valid_acc = 0.\n",
        "    valid_score = 0.\n",
        "\n",
        "    preds = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, trg in tqdm(valid_loader):\n",
        "            mask = src['attention_mask'].to(device)\n",
        "            input_id = src['input_ids'].squeeze(1).to(device)\n",
        "            segment_ids = src['token_type_ids'].squeeze(1).to(device)\n",
        "            trg = trg.long().to(device)  \n",
        "            \n",
        "            output = model(input_id, mask, segment_ids)\n",
        "            loss = criterion(output[0].view(-1,2), trg.view(-1))\n",
        "            valid_losses += loss.item()\n",
        "\n",
        "            acc = (output[0].argmax(dim=-1) == trg).sum().item()\n",
        "            valid_acc += acc\n",
        "\n",
        "            preds.extend(output[0].argmax(dim=-1).detach().cpu().numpy())\n",
        "            targets.extend(trg.detach().cpu().numpy())\n",
        "    \n",
        "    valid_score = f1_score(preds, targets)\n",
        "    print(\"recall\", recall_score(targets, preds), \"precision\", precision_score(targets, preds))\n",
        "    print(accuracy_score(targets, preds))\n",
        "    print(targets)\n",
        "    print(preds)\n",
        "    return valid_losses, valid_acc, valid_score, preds"
      ],
      "metadata": {
        "id": "i1b0gI4AzuHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "ZX4878jBs0bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'root_dir': '/content/drive/Othercomputers/내 컴퓨터/workspace/nlp_classification_',\n",
        "    \"result_dir\": '/content/drive/Othercomputers/내 컴퓨터/workspace/nlp_classification_/result',\n",
        "    'num_classes': 2,\n",
        "    \"max_seq_len\": None,\n",
        "    \"epochs\": 40,\n",
        "    \"seed\": 2022,\n",
        "    \"batch_size\": 16,\n",
        "    \"lr\": 1e-5,\n",
        "    \"warmup_proportion\": 0.1,\n",
        "    \"patience\": 10,\n",
        "    \"pretrained_model\": \"beomi/KcELECTRA-base\", # \"beomi/KcELECTRA-base\",  \n",
        "    \"architecture\": \"ElectraForSequenceClassification\", # \"ElectraForSequenceClassification\",\n",
        "    \"tokenizer_class\": \"ElectraTokenizer\", # \"ElectraTokenizer\",\n",
        "    'experiment_name': 'hate_pre+',\n",
        "    'n_split': 5,\n",
        "    'fold': 4\n",
        "}"
      ],
      "metadata": {
        "id": "RlrWfH1s5ekV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity_error()\n",
        "seed_everything(CFG['seed'])\n",
        "TOKENIZER_CLASSES = {\n",
        "    \"BertTokenizer\": BertTokenizer,\n",
        "    \"AutoTokenizer\": AutoTokenizer,\n",
        "    \"ElectraTokenizer\": ElectraTokenizer,\n",
        "    \"AlbertTokenizer\": AlbertTokenizer,\n",
        "    \"RobertaTokenizer\": RobertaTokenizer\n",
        "}\n",
        "BASE_MODELS = {\n",
        "    \"BertForSequenceClassification\": BertForSequenceClassification,\n",
        "    \"AutoModelForSequenceClassification\": AutoModelForSequenceClassification,\n",
        "    \"ElectraForSequenceClassification\": ElectraForSequenceClassification,\n",
        "    \"AlbertForSequenceClassification\": AlbertForSequenceClassification,\n",
        "    \"RobertaForSequenceClassification\": RobertaForSequenceClassification\n",
        "}\n",
        "\n",
        "TOKENIZER = TOKENIZER_CLASSES[CFG['tokenizer_class']].from_pretrained(CFG['pretrained_model'])\n",
        "TOKENIZER.add_tokens(['', ])\n",
        "\n",
        "print(\"number of GPUs: \", torch.cuda.device_count())\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "print(\"Does GPU exist? : \", use_cuda)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "DEBUG = True\n",
        "\n",
        "train_df = pd.read_csv(CFG['root_dir'] + '/train/train.csv', encoding = 'UTF-8-SIG')\n",
        "\n",
        "spacing = Spacing()\n",
        "train_df['title'] = train_df['title'].map(remove_repeat)\n",
        "train_df['comment'] = train_df['comment'].map(remove_repeat)\n",
        "# train_df['title'] = train_df['title'].map(remove_doubledot)\n",
        "# train_df['comment'] = train_df['comment'].map(remove_doubledot)\n",
        "# train_df['title'] = train_df['title'].map(clean_punc)\n",
        "# train_df['title'] = train_df['title'].map(remove_bracket)\n",
        "train_df['title'] = train_df['title'].map(remove_space)\n",
        "train_df['comment'] = train_df['comment'].map(remove_space)\n",
        "\n",
        "# preprocessing\n",
        "# processor = Mecab()\n",
        "# train_df['title'] = train_df['title'].map(preprocessing)\n",
        "# train_df['comment'] = train_df['comment'].map(preprocessing)\n",
        "\n",
        "train_df['label'] = train_df['hate'].map({'none':0, 'hate':1})\n",
        "\n",
        "train_data, valid_data = split_stratified_shuffle_split(train_df, CFG['fold'], CFG['n_split'], CFG['seed'])\n",
        "\n",
        "train_dataset = MyDataset(train_data, TOKENIZER, CFG['max_seq_len'])\n",
        "valid_dataset = MyDataset(valid_data, TOKENIZER, CFG['max_seq_len'])\n",
        "train_loader = DataLoader(train_dataset, batch_size=CFG['batch_size'], shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=CFG['batch_size'], shuffle=False) \n",
        "\n",
        "early_stopper = EarlyStopper(patience=CFG['patience'])\n",
        "model = MyModel(selected_layers=False).to(device)\n",
        "\n",
        "criterion = MyLoss().to(device)\n",
        "cosine_annealing_scheduler_arg1 = dict(\n",
        "    first_cycle_steps=len(train_dataset)//CFG['batch_size'] * CFG['epochs'],\n",
        "    cycle_mult=1.0,\n",
        "    max_lr=CFG['lr'],\n",
        "    min_lr=1e-08,\n",
        "    warmup_steps=len(train_dataset)//CFG['batch_size'] * 3,\n",
        "    gamma=0.9\n",
        ")\n",
        "cosine_annealing_scheduler_arg2 = dict(\n",
        "    first_cycle_steps=len(train_dataset)//CFG['batch_size'] * CFG['epochs'],\n",
        "    cycle_mult=1.0,\n",
        "    max_lr=CFG['lr'],\n",
        "    min_lr=1e-08,\n",
        "    warmup_steps=len(train_dataset)//CFG['batch_size'] * 3,\n",
        "    gamma=0.9\n",
        ")\n",
        "\n",
        "optimizer1 = optim.AdamW(model.parameters(), lr=0.000, weight_decay=0) # .model.electra.\n",
        "scheduler1 = CosineAnnealingWarmupRestarts(optimizer1, **cosine_annealing_scheduler_arg1)\n",
        "# optimizer2 = optim.AdamW(model.model.classifier.parameters(), lr=0.000, weight_decay=0)\n",
        "# scheduler2 = CosineAnnealingWarmupRestarts(optimizer2, **cosine_annealing_scheduler_arg2)"
      ],
      "metadata": {
        "id": "2GuMbjzgtMEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Start Training!')\n",
        "for i in range(CFG['epochs']):\n",
        "    print(f\"Epoch :\", i)\n",
        "    train_losses, train_match, train_score = train_one_epoch(\n",
        "        model, criterion, train_loader, optimizer1, scheduler1, device) # optimizer2, scheduler2, \n",
        "    \n",
        "    valid_losses, valid_match, valid_score, preds = valid_one_epoch(\n",
        "        model, criterion, valid_loader, device)\n",
        "    \n",
        "    print(f\"Train loss, acc, score: {train_losses / len(train_loader):.3f}, {train_match / len(train_loader.dataset):.3f}, {train_score:.3f}\")\n",
        "    print(f\"Valid loss, acc, score: {valid_losses / len(valid_loader):.3f}, {valid_match / len(valid_loader.dataset):.3f}, {valid_score:.3f}\")\n",
        "    early_stopper.check_early_stopping(valid_score)\n",
        "\n",
        "    # wandb_dict = {\n",
        "    #     'train loss': train_losses / len(train_loader),\n",
        "    #     'train score': train_match / len(train_loader.dataset),\n",
        "    #     'valid loss': valid_losses / len(valid_loader),\n",
        "    #     'valid score': valid_match / len(valid_loader.dataset),\n",
        "    #     'learning rate': scheduler.get_lr()[0]\n",
        "    # }\n",
        "\n",
        "    # wandb.log(wandb_dict)\n",
        "\n",
        "    print(\"learning rate :\", scheduler1.get_lr())\n",
        "\n",
        "    if early_stopper.save_model == True:\n",
        "        dic = {\n",
        "            'model':model.state_dict(),\n",
        "            'optimizer':optimizer1.state_dict(),\n",
        "            'scheduler':scheduler1.state_dict(),\n",
        "        }\n",
        "        torch.save(dic, \"/content/drive/MyDrive/Colab Notebooks\" + f\"/pth/{CFG['fold']}_best_{CFG['experiment_name']}.pth\")\n",
        "        print('save_model')\n",
        "\n",
        "    if early_stopper.stop:\n",
        "        break\n",
        "\n",
        "os.rename(\n",
        "    \"/content/drive/MyDrive/Colab Notebooks\" + f\"/pth/{CFG['fold']}_best_{CFG['experiment_name']}.pth\", \n",
        "    \"/content/drive/MyDrive/Colab Notebooks\" + f\"/pth/{CFG['fold']}_{early_stopper.max_score:.3f}_{CFG['experiment_name']}.pth\"\n",
        "    )"
      ],
      "metadata": {
        "id": "6eZ4OhOP0WJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/pth/0_best_hate.pth')['model'])\n",
        "valid_losses, valid_acc, valid_score, preds = valid_one_epoch(model, criterion, valid_loader, device)\n",
        "preds"
      ],
      "metadata": {
        "id": "xiKVKS3dCyDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = pd.DataFrame(valid_data)\n",
        "tmp['pred'] = preds\n",
        "tmp"
      ],
      "metadata": {
        "id": "0k8gV-lJDo37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "e4Q7Z1ais1ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터셋 불러오기\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    outputs = None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for test_input, _ in test_loader:\n",
        "\n",
        "            mask = test_input['attention_mask'].to(device)\n",
        "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
        "            segment_ids = test_input['token_type_ids'].squeeze(1).to(device)\n",
        "\n",
        "            output = model(input_id, mask, segment_ids)\n",
        "            if outputs is None:\n",
        "                outputs = output[0].detach().cpu().numpy()\n",
        "            else:\n",
        "                outputs = np.concatenate((outputs, output[0].detach().cpu().numpy()), axis=0)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "hecuF-lAqOGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = [\n",
        "           '/content/drive/MyDrive/Colab Notebooks/pth/0_840_hate_pre+.pth',\n",
        "           '/content/drive/MyDrive/Colab Notebooks/pth/1_848_hate_pre+.pth',\n",
        "           '/content/drive/MyDrive/Colab Notebooks/pth/2_841_hate_pre+.pth',\n",
        "           '/content/drive/MyDrive/Colab Notebooks/pth/3_0.847_hate_pre+.pth',\n",
        "           '/content/drive/MyDrive/Colab Notebooks/pth/4_0.830_hate_pre+.pth'\n",
        "]\n",
        "test_df = pd.read_csv(CFG['root_dir'] + '/test/test.csv')\n",
        "test_df['label'] = 0\n",
        "\n",
        "test_df['title'] = test_df['title'].map(remove_repeat)\n",
        "test_df['comment'] = test_df['comment'].map(remove_repeat)\n",
        "test_df['title'] = test_df['title'].map(remove_doubledot)\n",
        "test_df['comment'] = test_df['comment'].map(remove_doubledot)\n",
        "test_df['title'] = test_df['title'].map(clean_punc)\n",
        "test_df['title'] = test_df['title'].map(remove_bracket)\n",
        "test_df['title'] = test_df['title'].map(remove_space)\n",
        "test_df['comment'] = test_df['comment'].map(remove_space)\n",
        "\n",
        "test_dataset = MyDataset(test_df[['title', 'comment', 'label']].values, TOKENIZER, CFG['max_seq_len'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=CFG['batch_size'])"
      ],
      "metadata": {
        "id": "7Z0Bz3YNM0Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = None\n",
        "for weight in weights:\n",
        "    model = MyModel().to(device)\n",
        "    model.load_state_dict(torch.load(weight)['model'])\n",
        "\n",
        "    # test_df = pd.read_csv(CFG['root_dir'] + '/test/test.csv')\n",
        "    # test_df['label'] = 0\n",
        "    # test_dataset = MyDataset(test_df[['title', 'comment', 'label']].values, tokenizer = TOKENIZER, max_len= CFG['max_seq_len'])\n",
        "    # test_loader = DataLoader(test_dataset, batch_size=CFG['batch_size'])\n",
        "\n",
        "    if preds is None:\n",
        "        preds = test(model, test_loader)\n",
        "    else:\n",
        "        preds += test(model, test_loader)\n",
        "\n",
        "outputs = preds.argmax(axis=-1)\n"
      ],
      "metadata": {
        "id": "xPs3inxFwEaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = pd.read_csv('/content/drive/Othercomputers/내 컴퓨터/workspace/nlp_classification_/tmp.csv')\n",
        "tmp"
      ],
      "metadata": {
        "id": "ZiO3zGwfQBLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp['hate'] = outputs\n",
        "tmp['hate'] = tmp['hate'].map({0: 'none', 1: 'hate'})\n",
        "tmp['bias'] = tmp['bias'].map({0: 'none', 1: 'others', 2: 'gender'})\n",
        "tmp.to_csv('/content/drive/Othercomputers/내 컴퓨터/workspace/nlp_classification_/v02.csv', index=False)"
      ],
      "metadata": {
        "id": "we8dyWxBqwAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('./train.csv')\n",
        "# df['comment_title'] = df['comment'] + ' ' + df['title']\n",
        "# loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "FL_f2y4XRgN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}